------------------->#Prac 1 Practical 1 (Document_Indexing_and_Retrieval)
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
stopWords = stopwords.words('english')
print(stopWords)

document1 = "The quick brown fox jumped over the lazy dog."
document2 = "The lazy dog slept in the sun."

tokens1 = document1.lower().split()
tokens2 = document2.lower().split()

terms = list(set(tokens1 + tokens2))
print(tokens1)
print(tokens2)
print(terms)

inverted_index = {}
occ_num_doc1 = {}
occ_num_doc2 = {}

for term in terms:
    if term in stopWords:
        continue
    if term in tokens1:
        occ_num_doc1[term] = tokens1.count(term)
    if term in tokens2:
        occ_num_doc2[term] = tokens2.count(term)
print(occ_num_doc1)
print(occ_num_doc2)

for term in terms:
  if term in stopWords:
    continue
  documents = []
  if term in tokens1:
    documents.append("Document 1")
    occ_num_doc1[term] = tokens1.count(term)
  if term in tokens2:
    documents.append("Document 2")
    occ_num_doc2[term] = tokens2.count(term)
  inverted_index[term] = documents

print(occ_num_doc1)
print(occ_num_doc2)
print(inverted_index)

for term, documents in inverted_index.items():
  print(term, "->", end=" ")
  for doc in documents:
    if doc == "Document 1":
      print(f"{doc} ({occ_num_doc1.get(term, 0)}),", end= " ")
    else:
      print(f"{doc} ({occ_num_doc2.get(term, 0)}),", end= " ")



-------------------># #Practical 2
# a Implement the Boolean retrieval model and process queries. b) Implement the vector space model with TF-IDF weighting and cosine similarity.

documents = {
    1: "apple banana orange",
    2: "apple banana",
    3: "banana orange",
    4: "apple",
    5: "apple orange"
}

def create_inverted_index(documents):
    inverted_index = {}
    for doc_id, doc_text in documents.items():
        for term in doc_text.split():
            term = term.lower()
            inverted_index.setdefault(term, set()).add(doc_id)
    return inverted_index

inverted_index = create_inverted_index(documents)

def boolean_and(operands, index):
  result = set(index.get(operands[0], set()))
  for term in operands[1:]:
    result = result.intersection(index.get(term, set()))
  return list(result)

def boolean_or(operands, index):

  result = set()
  for term in operands:
    result.update(index.get(term, set()))
  return list(result)

def boolean_not(term, index, num_documents):
  all_docs = set(range(1, num_documents + 1))  # Create set of all document IDs (1 to num_documents)
  return list(all_docs - index.get(term, set()))  # Remove documents containing the term


#Example queries

queryl = ["apple", "banana"]
query2 = ["apple", "orange"]

#Performing Boolean model queries

result1 = boolean_and(queryl, inverted_index)
result2 = boolean_or(query2, inverted_index)
result3 = boolean_not("orange", inverted_index, len(documents))

print(inverted_index)

# Printing results
print("documents containing 'apple' and 'banana': ", result1)
print("Documents containing 'apple' or 'orange': ", result2)
print("Documents not containing 'orange': ", result3)


# Simple Code

from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
import numpy as np

train_set = ["The sky is blue.", "The sun is bright."]
test_set = ["The sun in the sky is bright."]

stopWords = stopwords.words('english')

# use TFidfVectoriser for tokenization, stop word removal and TF-IDF transform
vectorizer = TfidfVectorizer(stop_words=stopWords)

# create document-term matrices in one step for train and test sets

train_matrix = vectorizer.fit_transform(train_set)
test_matrix = vectorizer.transform(test_set)
print(train_matrix)
print(test_matrix)

# Calculate cosine similarity for each document in the test set against all documents

for test_vector in test_matrix.toarray():
  for train_vector in train_matrix.toarray():
    cosine_similarity = np.dot(test_vector, train_vector) / (np.linalg.norm(test_vector) * np.linalg.norm(train_vector))

    print(f"Cosine similarity between test document and train document: {cosine_similarity:.3f}")



-------------------># Practical 3
# a) Develop a spelling correction module using edit distance algorithms.
# b) Integrate the spelling correction module into an Information retrieval system.

def edit_distance(str1, str2):
    m, n = len(str1), len(str2)
    dp = [[0] * (n + 1) for _ in range(m + 1)]  # Create a dp table

    # Base cases: If one string is empty, the edit distance is the length of the other
    for i in range(m + 1):
        dp[i][0] = i
    for j in range(n + 1):
        dp[0][j] = j

    # Fill the DP table
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if str1[i - 1] == str2[j - 1]:  # If characters match, no edit needed
                dp[i][j] = dp[i - 1][j - 1]
            else:
                # Minimum of insert, delete, or substitute operations
                dp[i][j] = 1 + min(dp[i][j - 1], dp[i - 1][j], dp[i - 1][j - 1])

    return dp[m][n]  # Minimum edit distance at the bottom right corner

# Example Usage
str1 = "Sunday"
str2 = "Saturday"
min_edit_distance = edit_distance(str1, str2)
print('Edit distance between "{}" and "{}" is: {}'.format(str1, str2, min_edit_distance))

def editDistance(str1, str2, m, n):
    if m == 0:
        return n
    if n == 0:
        return m
    if str1[m - 1] == str2[n - 1]:
        return editDistance(str1, str2, m - 1, n - 1)
    return 1 + min(editDistance(str1, str2, m, n - 1),
                   editDistance(str1, str2, m - 1, n),
                   editDistance(str1, str2, m - 1, n - 1))

# Example Usage
str1 = "Sunday"
str2 = "Saturday"
min_edit_distance = editDistance(str1, str2, len(str1), len(str2))
print('Edit distance between "{}" and "{}" is: {}'.format(str1, str2, min_edit_distance))


# Dictionary of correctly spelled words
dictionary = {"search", "engine", "example"}

# Spelling correction function
def spell_correct(word):
    # Check if the word is in the dictionary
    if word in dictionary:
        return word

    # If not in the dictionary, find the closest word based on edit distance
    closest_word = min(dictionary, key=lambda x: edit_distance(word, x))

    # If the closest word is within edit distance 1, return it as the corrected word
    if edit_distance(word, closest_word) == 1:
        return closest_word

    # If no close match is found, return the original word
    return word

-------------------># Process query function
def process_query(query):
    # Split the query into individual words
    words = query.split()

    # Correct the spelling of each word in the query
    corrected_words = [spell_correct(word) for word in words]

    # Join the corrected words back into a single string
    corrected_query = ' '.join(corrected_words)

    return corrected_query

# Example Usage
def main():
    # User query
    query = "serch enjine exampl"
    print(f"Original Query: {query}")
    # Process the query and correct spelling
    corrected_query = process_query(query)

    # Print corrected query
    print("Corrected Query:", corrected_query)

if __name__ == "__main__":
    main()



-------------------># Practical 4 (Calculate precision, recall and F-measure for a given set of retrieval results)

# A) Precision, Recall, F-measure

def calculate_metrics(retrieved_set, relevant_set):
  true_positive = len(retrieved_set.intersection(relevant_set))
  false_positive = len(retrieved_set.difference(relevant_set))
  false_negative = len(relevant_set.difference(retrieved_set))

  print("True Positive: ", true_positive, "\nFalse Positive: ", false_positive, "\nFalse Negative: ", false_negative, "\n")

  precision = true_positive / (true_positive + false_positive)
  recall = true_positive / (true_positive + false_negative)
  f_measure = 2 * precision * recall / (precision + recall)
  return precision, recall, f_measure

retrieved_set = set(["doc1", "doc2", "doc3"])  #predicted set
relevant_set = set(["doc1", "doc4"])           #actually needed set (relevant)
precision, recall, f_measure = calculate_metrics(retrieved_set, relevant_set)

print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F-measure: {f_measure}")


# B) Average precision

from sklearn.metrics import average_precision_score
y_true = [0, 1, 1, 0, 1, 1]   #binary prediction
y_scores = [0.1, 0.4, 0.35, 0.8, 0.65, 0.9]   #model's estimation score
average_precision = average_precision_score(y_true, y_scores)
print(f'Average precision-recall score: {average_precision}')


------------------->Practical 5
a) Implement a text classification algorithm (eg. Naive Bayes or Support Vector Machines).
b) Train the classifier on a labelled data set and evaluate its performance.


import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

df = pd.read_csv(r"IRPrac4Dataset.csv - Sheet1.csv")
data = df["covid"]+ " " +df["fever"]

x = data.astype(str)
y = df['flu']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

vectorizer = CountVectorizer()
x_train_counts = vectorizer.fit_transform(x_train)
x_test_counts = vectorizer.transform(x_test)

classifier = MultinomialNB()
classifier.fit(x_train_counts, y_train)

data1 = pd.read_csv(r"IRPrac4Dataset.csv - Sheet1.csv")
new_data=data1["covid"] + " " + data1["fever"]
new_data_counts = vectorizer.transform(new_data.astype(str))

predictions = classifier.predict(new_data_counts)
new_data = predictions

print(new_data)

accuracy = accuracy_score(y_test, classifier.predict(x_test_counts))

print(f"\nAccuracy: {accuracy:.2f}")
print("Classification Report: ")
print(classification_report(y_test, classifier.predict(x_test_counts)))


-------------------> Practical 6 (Clustering for Information Retrieval:)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans


documents = ['Cats are known for their agility and grace.',#cat
             'Dogs are often called mans best friend.',#dog
             'Some dogs are trained to assist people with disabilities',#dog
             'The sun rises in the east and sets in the west',#sun
             'Many cats enjoy climbing trees and chasing toys',#cat
]

#Create a TfidfVectorizer object

vectorizer = TfidfVectorizer(stop_words='english')

#Learn vocabulary and idf from training set.

X = vectorizer.fit_transform(documents)

#Perform k-means clusteering

kmeans = KMeans(n_clusters=3,random_state=0).fit(X)

#Print cluster labels for each document

print(kmeans.labels_)


----------->Prac7 WebCrawler

import requests
from bs4 import BeautifulSoup
import time 
from urllib.parse import urljoin,urlparse
from urllib.robotparser import RobotFileParser

def get_html(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.text
    except requests.exceptions.HTTPError as errh:
        print(f'HTTP Error: {errh}')
    except requests.exceptions.RequestException as err:
        print(f'Request Error: {err}')
    return None

def save_robots_txt(url):
    try:
        robots_url = urljoin(url, '/robots.txt')
        robots_content = get_html(robots_url)
        if robots_content:
            with open('robots.txt', 'wb') as file:
                file.write(robots_content.encode('utf-8-sig'))
    except Exception as e:
        print(f'Error saving robots.txt: {e}')

def load_robots_txt():
    try:
        with open('robots.txt', 'rb') as file:
            return file.read().decode('utf-8-sig')
    except FileNotFoundError:
        return None

def extract_links(html, base_url):
    soup = BeautifulSoup(html, 'html.parser')
    links = []
    for link in soup.find_all('a', href=True):
        absolute_url = urljoin(base_url, link['href'])
        links.append(absolute_url)
    return links

def is_allowed_by_robots(url, robots_content):
    parser = RobotFileParser()
    parser.parse(robots_content.split('\n'))
    return parser.can_fetch('*', url)

def crawl(start_url, max_depth=3, delay=1):
    visited_urls = set()
    save_robots_txt(start_url)
    robots_content = load_robots_txt()
    if not robots_content:
        print('Unable to retrieve robots.txt. Crawling without restrictions')
    recursive_crawl(start_url, 1, robots_content, max_depth, visited_urls)

def recursive_crawl(url, depth, robots_content, max_depth, visited_urls):
    if depth > max_depth or url in visited_urls or not is_allowed_by_robots(url, robots_content):
        return
    visited_urls.add(url)
    html = get_html(url)
    if html:
        print(f"Crawling {url}")
        links = extract_links(html, url)
        for link in links:
            recursive_crawl(link, depth + 1, robots_content, max_depth, visited_urls)

# Example Usage
crawl('https://wikipedia.com', max_depth=2, delay=2)


---------->Practical 8 PageRank

import numpy as np

def page_rank(graph, damping_factor=0.85, max_iterations=100, tolerance=1e-6):
  num_nodes = len(graph)
  page_ranks = np.ones(num_nodes) / num_nodes
  for _ in range(max_iterations):
    prev_page_ranks = np.copy(page_ranks)
    for node in range(num_nodes):
      incoming_links = [i for i, v in enumerate(graph) if node in v]
      if not incoming_links:
        continue

      page_ranks[node] = (1 - damping_factor) / num_nodes + damping_factor * sum(prev_page_ranks[incoming_link] / len(graph[incoming_link]) for incoming_link in incoming_links)

      if np.linalg.norm(page_ranks - prev_page_ranks, 2) < tolerance:
        break
  return page_ranks

if __name__ == '__main__':
  web_graph = [
    [1, 2],
    [0, 2],
    [0, 1],
    [1, 2]
  ]

  result = page_rank(web_graph)
  if result is not None:
    for i, pr in enumerate(result):
      print(f"Page{i}:{pr}")
  else:
    print("Pagerank values are None")


